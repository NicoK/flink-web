---
layout: post
title: "Building a Fraud Detection System with Runtime Rules Updates on Apache Flink"
date: 2019-11-19T12:00:00.000Z
authors:
- alex:
  name: "Alexander Fedulov"
  twitter: "afedulov"
excerpt:
categories: news
---

In this blog post you will learn about three Flink patterns for building streaming applications:

 - Dynamic application logic updates
 - Dynamic data partitioning (shuffle), controlled at runtime
 - Low latency alerting based on windowed calculations directly via Flink state (not API)

Those patterns might not be immediately obvious from the framework's documentation, however, they provide important building blocks to fulfil versatile business requirements.

**Dynamic application logic updates** allow to modify Flink job execution at runtime, without stopping and resubmitting the code.     
**Dynamic data partitioning** enables the ability to configure how events are being shuffled and grouped within the cluster at runtime.    
**Custom windows management** demonstrates how you can utilize the low level [ProcessFunction API](https://ci.apache.org/projects/flink/flink-docs-stable/dev/stream/operators/process_function.html), when [Windows API](https://ci.apache.org/projects/flink/flink-docs-stable/dev/stream/operators/windows.html) is not exactly matching your requirements. Specifically, you will learn how to implement low latency alerting on windows and how to limit state growth with timers.

Those patterns are put together and demonstrated based on an example of a Fraud Detection application - a common use case for Apache Flink. This demo application consumes a stream of financial transactions and evaluates a set of rules against it. Those rules can be added and removed at runtime, without restarting the job.

<center>
<img src="{{ site.baseurl }}/img/blog/2019-11-19-demo-fraud-detection/ui.png" width="800px" alt="Figure 1: Demo UI"/>
<br/>
<i><small>Figure 1: Demo UI</small></i>
</center>
<br/>



This blogpost will instruct how to run the demo locally, will describe it's components and their interactions. We will look in the details into the first pattern: Dynamic Data Partitioning.

If you are only interested in learning about a specific pattern, you can jump to the respective separate section (//TODO: ? depends on break up of further blogposts):  
 - Link 1  
 - Link 2  
 - Link 3  


### Setup

This blogpost is accompanied by a github repository:

[](https://github.com/afedulov/fraud-detection-demo)

Setup is dockerized and includes the following components:  

 - Apache Kafka (broker) with ZooKeeper
 - Apache Flink ([application cluster](https://ci.apache.org/projects/flink/flink-docs-stable/concepts/glossary.html#flink-application-cluster))
 - Demo WebApp  

#### Requirements:
Demo is bundled in a self-contained package. In order to build it from sources you will need:

 - git
 - docker
 - docker-compose

 Recommended resources allocated to Docker:

 - 4 CPUs
 - 8GB RAM

 You can checkout the repository and run the demo locally.

#### How to run:

 Execute the following commands to build the project from sources and start all required services, including Apache Flink and Apache Kafka clusters.

```bash
git clone https://github.com/afedulov/fraud-detection-demo
docker build -t demo-fraud-webapp:latest -f webapp/webapp.Dockerfile webapp/
docker build -t flink-job-fraud-demo:latest -f flink-job/Dockerfile flink-job/
docker-compose -f docker-compose-local-job.yaml up
```

__Note__: Dependencies are stored in a cached Docker layer. If you later only modify the source code, not the dependencies, you can expect significantly shorter packaging times for the subsequent builds.

When all components are up and running, go to `localhost:5656` in your browser.

__Note__: you might need to change exposed ports in _docker-compose-local-job.yaml_ in case of collisions.

The demo comes with a set of predefined rules. You can simply click the _Start_ button and after some time you should observe alerts displayed on the right side of the screen. Those alerts are the results of Flink evaluating the generated transactions stream against the predefined rules.

 The sample application consists of three main parts:  

 1. Fraud Detection application (Apache Flink)  
 1. Frontend (React)  
 1. Backend (SpringBoot)  


 Backend includes a Transactions Generator, which sends emulated transactions to Flink via Kafka. Whenever a new rule definition is created in the UI, the backend also pushes it to Flink via a separate topic. Alerts generated by Flink are consumed by the Backend and relayed to the UI via WebSockets.

 <center>
 <img src="{{ site.baseurl }}/img/blog/2019-11-19-demo-fraud-detection/architecture.png" width="800px" alt="Figure 2: Demo Components"/>
 <br/>
 <i><small>Figure 2: Demo Components</small></i>
 </center>
 <br/>


### Dynamic Data Partitioning

Let us start with formulating a sample rule definition for our fraud detection system as a functional requirement:  

"Whenever the **sum** of accumulated **payment amount** from the same **beneficiary** to the same **payee** within the **duration of a week** is **greater** than **1 000 000 $** - fire an alert."

From this formulation we can extract the following parameters that we would like to be able to specify in a system which allows flexibility in rules definition:  

1. Aggregation field (payment amount)  
1. Grouping fields (beneficiary + payee)  
1. Aggregation function (sum)  
1. Window duration (1 week)  
1. Limit (1 000 000)  
1. Limit operator (greater)  

Accordingly, we will use the following simple JSON format to define the aforementioned parameters.

Examples JSON:

```json  
{
  "ruleId": 1,
  "ruleState": "ACTIVE",
  "groupingKeyNames": ["beneficiaryId", "payeeId"],
  "aggregateFieldName": "paymentAmount",
  "aggregatorFunctionType": "SUM",
  "limitOperatorType": "GREATER",
  "limit": 1000000,
  "windowMinutes": 10080
}
```

At this point it is important to understand that **`groupingKeyNames`** determine the actual physical grouping of events - all Transactions with the same values of specified parameters (e.g. _beneficiary #25 -> payee #12_ have to be aggregated in the same parallel instance of the evaluating operator. Naturally, the process of distributing data in such manner in Flink's API is realised by a `keyBy` function.

Although most examples in the [documentation](https://ci.apache.org/projects/flink/flink-docs-stable/dev/api_concepts.html#define-keys-using-field-expressions) use specific fixed events' fields, nothing prevents us from extracting them in a more dynamic fashion, based on the specifications of the rules. For this we will need one additional preparation step before invoking the `keyBy` function.

Let's look at a high level how our main processing pipeline might look like:

```java
DataStream<Alert> alerts =
    transactions
        .process(new DynamicKeyFunction())
        .keyBy(/* some key selector */);
        .process(/* actual calculations and alerting */)
```

Given a set of predefined rules in the first step of the processing pipeline, we would like to iterate over them and prepare every event to be dispatched to a respective aggregating instance. This is what is done in `DynamicKeyFunction`:

```java
public class DynamicKeyFunction
    extends ProcessFunction<Transaction, Keyed<Transaction, String, Integer>> {
   ...
  /* Simplified */
  List<Rule> rules = /* Rules that are initialized somehow.
                        Details will be discussed in a separate section. */;

  @Override
  public void processElement(
      Transaction event,
      Context ctx,
      Collector<Keyed<Transaction, String, Integer>> out) {

      for (Rule rule :rules) {
       out.collect(
           new Keyed<>(
               event,
               KeysExtractor.getKey(rule.getGroupingKeyNames(), event),
               rule.getRuleId()));
      }
  }
  ...
}
``` 
 `KeysExtractor.getKey()` uses reflection to extract required values of `groupingKeyNames` fields from events and combines them as a single concatenated String key, e.g `{beneficiaryId=25;payeeId=12}`.

Notice that a wrapper class `Keyed` with the following signature was introduced as the output type of `DynamicKeyFunction`:  

```java   
public class Keyed<IN, KEY, ID> {
  private IN wrapped;
  private KEY key;
  private ID id;

  ...
  public KEY getKey(){
      return key;
  }
}
```

Where `wrapped` is the original Transaction event data, `key` is the result of using `KeysExtractor` and `id` is the ID of the Rule which caused the dispatch of the event according to this Rule's grouping logic.

Events of this type will be the input to the `keyBy` function of the main processing pipeline. This allows us to use a simple lambda-expression in place of a [`KeySelector`](https://ci.apache.org/projects/flink/flink-docs-stable/dev/api_concepts.html#define-keys-using-key-selector-functions) as the final step of implementing dynamic data shuffle.

```java
DataStream<Alert> alerts =
    transactions
        .process(new DynamicKeyFunction())
        .keyBy((keyed) -> keyed.getKey());
        .process(/* actual calculations and alerting */)
```

Effectively we are forking events for parallel evaluation in the Flink cluster. There is a certain amount of data duplication which is required for making use of Flink's parallel model of execution. In a real-life scenario, an additional layer for filtering and propagating only those Transaction fields which are actually needed for evaluation of specific rules could be applied to decrease this impact.

<center>
<img src="{{ site.baseurl }}/img/blog/2019-11-19-demo-fraud-detection/shuffle.png" width="800px" alt="Figure 3: Events Shuffle"/>
<br/>
<i><small>Figure 3: Events Shuffle</small></i>
</center>
<br/>


Building a fully fledged DSL and a rules engine is not the focus of this post, hence this part is be kept to a minimum of what is required to show case intended functionality. You can, however, think about implementing more sophisticated rules, including filtering of certain events, rules chaining and other more advanced scenarios.

In the next part of this blogpost we will look into how those rules sets make it into Flink and how they can be added and removed at runtime (Dynamic Application Updates pattern).
