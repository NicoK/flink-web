---
layout: post
title: "Building a Fraud Detection System with Runtime Rules Updates on Apache Flink"
date: 2019-11-19T12:00:00.000Z
authors:
- alex:
  name: "Alexander Fedulov"
  twitter: "afedulov"
excerpt:
categories: news
---

In this series of blog posts you will learn about three powerful Flink patterns for building streaming applications:

 - Dynamic updates of application logic
 - Dynamic data partitioning (shuffle), controlled at runtime
 - Low latency alerting based on custom window logic (not using the Window API)

These patterns expand the possibilities of what is possible with statically defined data flows and therefore provide important building blocks to fulfil versatile business requirements.

**Dynamic updates of application logic** allow Flink jobs to change at runtime, without downtime from stopping and resubmitting the code.  
<br>
**Dynamic data partitioning** enables the ability to change how events are being distributed and grouped by the `keyBy()` operator in Flink at runtime. Such functionality often becomes a natural requirement when building jobs with dynamically reconfigurable application logic.  
<br>
**Custom window management** demonstrates how you can utilize the low level [ProcessFunction API](https://ci.apache.org/projects/flink/flink-docs-stable/dev/stream/operators/process_function.html), when [Window API](https://ci.apache.org/projects/flink/flink-docs-stable/dev/stream/operators/windows.html) is not exactly matching your requirements. Specifically, you will learn how to implement low latency alerting on windows and how to limit state growth with timers.    

The patterns are built up on top of core Flink functionality, however they might not be immediately obvious from the framework's documentation as it is not trivial to explain and give motivation behind them without a concrete example use case. For this reason we will look at the details of the patterns on an example of building an application which represents a common a use case for Apache Flink - a _Fraud Detection_ engine.
We hope this series will put these powerful ideas into your tool belt and enable you to tackle new interesting use cases with Apache Flink.

In the first blog post of the series we will look at the high level architecture of the demo application, describe its components and their interactions. We will proceed with a deep dive into the implementation details of the first pattern in the series - **dynamic data partitioning**.

You will be able to run the full Fraud Detection Demo application locally and look into the details of the implementation by using accompanying GitHub repository.


### Fraud Detection Demo

##### Setup

If you want to see the complete code ....
This blogpost is accompanied by a github repository:

[https://github.com/afedulov/fraud-detection-demo](https://github.com/afedulov/fraud-detection-demo)

You can checkout the repository and run the demo locally. Setup is dockerized and includes the following components:  

 - Apache Kafka (broker) with ZooKeeper
 - Apache Flink ([application cluster](https://ci.apache.org/projects/flink/flink-docs-stable/concepts/glossary.html#flink-application-cluster))
 - The Fraud Detection App  

The demo comes with a set of predefined rules. You can simply click the _Start_ button and after some time you should observe alerts displayed on the right side of the screen. Those alerts are the results of Flink evaluating the generated transactions stream against the predefined rules.


The high level goal of our demo Fraud Detection engine is to consume a stream of financial transactions and evaluate a set of rules against it. Such rules can be added and removed at runtime, without restarting the job.

Our sample fraud detection application consists of three main components:

 1. Fraud Detection application (Apache Flink)  
 1. Frontend (React)  
 1. Backend (SpringBoot)  

 <center>
 <img src="{{ site.baseurl }}/img/blog/2019-11-19-demo-fraud-detection/ui.png" width="800px" alt="Figure 1: Demo UI"/>
 <br/>
 <i><small>Figure 1: Fraud Detection Demo UI</small></i>
 </center>
 <br/>

 Backend includes a Transactions Generator, which sends emulated transactions to Flink via Kafka. Whenever a new rule definition is created in the UI, the backend also pushes it to Flink via a separate topic. Alerts generated by Flink are consumed by the Backend and relayed to the UI via WebSockets.

 <center>
 <img src="{{ site.baseurl }}/img/blog/2019-11-19-demo-fraud-detection/architecture.png" width="800px" alt="Figure 2: Demo Components"/>
 <br/>
 <i><small>Figure 2: Fraud Detection Demo Components</small></i>
 </center>
 <br/>


### Dynamic Data Partitioning

Let us start with formulating a sample rule definition for our fraud detection system as a functional requirement:  

"Whenever the **sum** of accumulated **payment amount** from the same **beneficiary** to the same **payee** within the **duration of a week** is **greater** than **1 000 000 $** - fire an alert."

From this formulation we can extract the following parameters that we would like to be able to specify in a system which allows flexibility in rules definition:  

1. Aggregation field (payment amount)  
1. Grouping fields (beneficiary + payee)  
1. Aggregation function (sum)  
1. Window duration (1 week)  
1. Limit (1 000 000)  
1. Limit operator (greater)  

Accordingly, we will use the following simple JSON format to define the aforementioned parameters.

Examples JSON:

```json  
{
  "ruleId": 1,
  "ruleState": "ACTIVE",
  "groupingKeyNames": ["beneficiaryId", "payeeId"],
  "aggregateFieldName": "paymentAmount",
  "aggregatorFunctionType": "SUM",
  "limitOperatorType": "GREATER",
  "limit": 1000000,
  "windowMinutes": 10080
}
```

At this point it is important to understand that **`groupingKeyNames`** determine the actual physical grouping of events - all Transactions with the same values of specified parameters (e.g. _beneficiary #25 -> payee #12_ have to be aggregated in the same parallel instance of the evaluating operator. Naturally, the process of distributing data in such manner in Flink's API is realised by a `keyBy()` function.

Although most examples in the [documentation](https://ci.apache.org/projects/flink/flink-docs-stable/dev/api_concepts.html#define-keys-using-field-expressions) use specific fixed events' fields, nothing prevents us from extracting them in a more dynamic fashion, based on the specifications of the rules. For this we will need one additional preparation step before invoking the `keyBy()` function.

Let's look at a high level how our main processing pipeline might look like:

```java
DataStream<Alert> alerts =
    transactions
        .process(new DynamicKeyFunction())
        .keyBy(/* some key selector */);
        .process(/* actual calculations and alerting */)
```

Given a set of predefined rules in the first step of the processing pipeline, we would like to iterate over them and prepare every event to be dispatched to a respective aggregating instance. This is what is done in `DynamicKeyFunction`:

```java
public class DynamicKeyFunction
    extends ProcessFunction<Transaction, Keyed<Transaction, String, Integer>> {
   ...
  /* Simplified */
  List<Rule> rules = /* Rules that are initialized somehow.
                        Details will be discussed in a separate section. */;

  @Override
  public void processElement(
      Transaction event,
      Context ctx,
      Collector<Keyed<Transaction, String, Integer>> out) {

      for (Rule rule :rules) {
       out.collect(
           new Keyed<>(
               event,
               KeysExtractor.getKey(rule.getGroupingKeyNames(), event),
               rule.getRuleId()));
      }
  }
  ...
}
```
 `KeysExtractor.getKey()` uses reflection to extract required values of `groupingKeyNames` fields from events and combines them as a single concatenated String key, e.g `{beneficiaryId=25;payeeId=12}`.

Notice that a wrapper class `Keyed` with the following signature was introduced as the output type of `DynamicKeyFunction`:  

```java   
public class Keyed<IN, KEY, ID> {
  private IN wrapped;
  private KEY key;
  private ID id;

  ...
  public KEY getKey(){
      return key;
  }
}
```

Where `wrapped` is the original Transaction event data, `key` is the result of using `KeysExtractor` and `id` is the ID of the Rule which caused the dispatch of the event according to this Rule's grouping logic.

Events of this type will be the input to the `keyBy()` function of the main processing pipeline. This allows us to use a simple lambda-expression in place of a [`KeySelector`](https://ci.apache.org/projects/flink/flink-docs-stable/dev/api_concepts.html#define-keys-using-key-selector-functions) as the final step of implementing dynamic data shuffle.

```java
DataStream<Alert> alerts =
    transactions
        .process(new DynamicKeyFunction())
        .keyBy((keyed) -> keyed.getKey());
        .process(/* actual calculations and alerting */)
```

Effectively we are forking events for parallel evaluation in the Flink cluster. There is a certain amount of data duplication which is required for making use of Flink's parallel model of execution. In a real-life scenario, an additional layer for filtering and propagating only those Transaction fields which are actually needed for evaluation of specific rules could be applied to decrease this impact.

<center>
<img src="{{ site.baseurl }}/img/blog/2019-11-19-demo-fraud-detection/shuffle.png" width="800px" alt="Figure 3: Events Shuffle"/>
<br/>
<i><small>Figure 3: Events Shuffle</small></i>
</center>
<br/>


Building a fully fledged DSL and a rules engine is not the focus of this post, hence this part is be kept to a minimum of what is required to show case intended functionality. You can, however, think about implementing more sophisticated rules, including filtering of certain events, rules chaining and other more advanced scenarios.

In the next part of this series of blog posts, we will look into how these rule sets make it into Flink and how they can be added and removed at runtime (the Dynamic Application Updates pattern).
